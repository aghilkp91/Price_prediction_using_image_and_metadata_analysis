{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DeepImageUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/2578512813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/Price_regression_using_CBIR/Retrieval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDeepImageUtils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mIU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DeepImageUtils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "from platform import platform\n",
    "from glob import iglob\n",
    "import math\n",
    "#supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import median\n",
    "from scipy.stats import norm\n",
    "import re\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import iplot\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "# from wordcloud import WordCloud\n",
    "import emoji\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/Price_regression_using_CBIR/Retrieval')\n",
    "import DeepImageUtils as IU\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]\n",
    "test_path = current_folder + '/test_records.csv'\n",
    "validation_path = current_folder + '/validation_records.csv'\n",
    "train_path = current_folder + '/train_records.csv'\n",
    "test_df = pd.read_csv(Path(test_path), error_bad_lines=False)\n",
    "validation_df = pd.read_csv(Path(validation_path), error_bad_lines=False)\n",
    "train_df = pd.read_csv(Path(train_path), error_bad_lines=False)\n",
    "#For this we merge training and valiadtion\n",
    "train_df = pd.concat([train_df, validation_df])\n",
    "\n",
    "train_df = train_df.drop('Unnamed: 0', 1)\n",
    "train_df = train_df.drop('Unnamed: 0.1', 1)\n",
    "test_df = test_df.drop('Unnamed: 0', 1)\n",
    "test_df = test_df.drop('Unnamed: 0.1', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniq_id                                1b04ee78c86c7ac1f9df869a91dd5747\n",
      "brand                                                          Fabindia\n",
      "care_instructions                             Viscose&nbsp; | Hand-wash\n",
      "dominant_material                                               Viscose\n",
      "title                 Fabindia Rust Red & Black Handblock Print Butt...\n",
      "actual_color                                         Red | Black | Rust\n",
      "dominant_color                                                     Rust\n",
      "product_type                                                      Shrug\n",
      "images                http://assets.myntassets.com/v1/assets/images/...\n",
      "product_details       Rust red and black printed longline shrug with...\n",
      "complete_the_look     A fantastic cover-up for when the temperature ...\n",
      "price                                                              2590\n",
      "gender                                                            Women\n",
      "specifications        Occasion : Ethnic | Main Trend : Ethnic Print ...\n",
      "category_0                                                     Clothing\n",
      "category_1                                                        Shrug\n",
      "image_0               http://assets.myntassets.com/v1/assets/images/...\n",
      "image_1                http://assets.myntassets.com/v1/assets/images...\n",
      "image_2                http://assets.myntassets.com/v1/assets/images...\n",
      "luxurious_brands                                                      0\n",
      "expensive_brands                                                      0\n",
      "cheap_brands                                                          1\n",
      "care_instruction_0                                        Viscose&nbsp;\n",
      "care_instruction_1                                            Hand-wash\n",
      "Name: 1280, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.iloc[1280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punchuations(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contract_words(text):\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(data, cols):\n",
    "    for col in cols:\n",
    "\n",
    "        processed_data = []\n",
    "\n",
    "        for sentence in data[col].values:\n",
    "            sent = remove_contract_words(sentence)\n",
    "            # sent = sentence\n",
    "            sent = re.sub(\"\\s+\", \" \", sent)\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            sent = give_emoji_free_text(sent)\n",
    "            sent = remove_punchuations(sent)\n",
    "            sent = remove_stopwords(sent)\n",
    "            processed_data.append(sent.lower().strip())\n",
    "\n",
    "        data[col] = processed_data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_path = globals()['_dh'][0] + '/total_features.npz'\n",
    "loaded_features = np.load(category_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(loaded_features['arr_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86042"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][2]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_ids = [feature[2] for feature in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86042"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coun = Counter(uniq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(coun.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'463018956bd93929f483b8a179f9e688'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_dict = {key: None for key in keys}\n",
    "train_df = pd.read_csv(globals()['_dh'][0] + \"/train_records.csv\")\n",
    "validation_df = pd.read_csv(globals()['_dh'][0] + \"/validation_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463018956bd93929f483b8a179f9e688\n",
      "here\n",
      "4792\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "iAt based indexing can only have integer indexers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/22194483.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uniq_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'image_feature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#             train_df.loc[loc, 'image_feature'] = feature[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# train_df['image_feature'] = feature[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Aghil/Radboud/Thesis/Codes/Price_regression_using_CBIR/.venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough indexers for scalar access (setting)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Aghil/Radboud/Thesis/Codes/Price_regression_using_CBIR/.venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_key\u001b[0;34m(self, key, is_setter)\u001b[0m\n\u001b[1;32m   2297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iAt based indexing can only have integer indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: iAt based indexing can only have integer indexers"
     ]
    }
   ],
   "source": [
    "feature_dict = dict()\n",
    "for feature in features:\n",
    "    if feature[2] in feature_dict:\n",
    "#         if feature[1] == feature_dict[feature[2]]:\n",
    "#             continue\n",
    "        feature_dict[feature[2]].append(feature[1])\n",
    "    else:\n",
    "        feature_dict[feature[2]] = [feature[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                            0\n",
       "Unnamed: 0.1                                                       7601\n",
       "uniq_id                                e87417d6278a1d04085694d5b46a50d4\n",
       "brand                                                             Libas\n",
       "care_instructions                                     RayonMachine-wash\n",
       "dominant_material                                                 Rayon\n",
       "title                 Libas Women Mustard Yellow Hem Design Wide Leg...\n",
       "actual_color                                           Yellow | Mustard\n",
       "dominant_color                                                  Mustard\n",
       "product_type                                           Wide Leg Palazzo\n",
       "images                http://assets.myntassets.com/v1/assets/images/...\n",
       "product_details       A pair of mustard yellow solid woven wide leg ...\n",
       "complete_the_look     Show off your fun side this weekend with these...\n",
       "price                                                               999\n",
       "gender                                                            Women\n",
       "specifications                                                      NaN\n",
       "category_0                                                     Clothing\n",
       "category_1                                                     Palazzos\n",
       "image_0               http://assets.myntassets.com/v1/assets/images/...\n",
       "image_1                http://assets.myntassets.com/v1/assets/images...\n",
       "image_2                http://assets.myntassets.com/v1/assets/images...\n",
       "luxurious_brands                                                      0\n",
       "expensive_brands                                                      0\n",
       "cheap_brands                                                          1\n",
       "care_instruction_0                                    RayonMachine-wash\n",
       "care_instruction_1                                                  NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/630706391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'463018956bd93929f483b8a179f9e688'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'463018956bd93929f483b8a179f9e688'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "feature_dict['463018956bd93929f483b8a179f9e688'][1] == feature_dict['463018956bd93929f483b8a179f9e688'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(globals()['_dh'][0] + \"/train_records.csv\")\n",
    "feature_dict = dict()\n",
    "for feature in features:\n",
    "        feature_dict[feature[2]] = [feature[1]]\n",
    "train_features_in_order = []\n",
    "for index, row in X_train.iterrows():\n",
    "        train_features_in_order.insert(index, feature_dict[row.uniq_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                            0\n",
       "Unnamed: 0.1                                                       7601\n",
       "uniq_id                                e87417d6278a1d04085694d5b46a50d4\n",
       "brand                                                             Libas\n",
       "care_instructions                                     RayonMachine-wash\n",
       "dominant_material                                                 Rayon\n",
       "title                 Libas Women Mustard Yellow Hem Design Wide Leg...\n",
       "actual_color                                           Yellow | Mustard\n",
       "dominant_color                                                  Mustard\n",
       "product_type                                           Wide Leg Palazzo\n",
       "images                http://assets.myntassets.com/v1/assets/images/...\n",
       "product_details       A pair of mustard yellow solid woven wide leg ...\n",
       "complete_the_look     Show off your fun side this weekend with these...\n",
       "price                                                               999\n",
       "gender                                                            Women\n",
       "specifications                                                      NaN\n",
       "category_0                                                     Clothing\n",
       "category_1                                                     Palazzos\n",
       "image_0               http://assets.myntassets.com/v1/assets/images/...\n",
       "image_1                http://assets.myntassets.com/v1/assets/images...\n",
       "image_2                http://assets.myntassets.com/v1/assets/images...\n",
       "luxurious_brands                                                      0\n",
       "expensive_brands                                                      0\n",
       "cheap_brands                                                          1\n",
       "care_instruction_0                                    RayonMachine-wash\n",
       "care_instruction_1                                                  NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.01092354, 0.01591033, 0.        , ..., 0.027866  , 0.00717207,\n",
       "        0.0076995 ], dtype=float32)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_in_order[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/2209202817.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'e87417d6278a1d04085694d5b46a50d4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/2209202817.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'e87417d6278a1d04085694d5b46a50d4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float32' has no len()"
     ]
    }
   ],
   "source": [
    "[len(a) for a in feature_dict['e87417d6278a1d04085694d5b46a50d4'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing metadata processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    # return emoji.get_emoji_regexp().sub(r'', text)\n",
    "    pattern = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punchuations(sentence):\n",
    "    # return text.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "    regular_punct = list(string.punctuation)\n",
    "\n",
    "    for punc in regular_punct:\n",
    "        if punc in sentence:\n",
    "            sentence = sentence.replace(punc, ' ')\n",
    "\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_contract_words(text):\n",
    "    try:\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    except Exception as exp:\n",
    "        print(exp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # return ' '.join([word for word in text if word not in stopwords.words('english')])\n",
    "    return ' '.join(e for e in text.split() if e not in STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(data, cols):\n",
    "    # print(data.columns.values.tolist())\n",
    "    for col in cols:\n",
    "\n",
    "        processed_data = []\n",
    "\n",
    "        for sentence in data[col].values:\n",
    "            sent = remove_contract_words(sentence)\n",
    "            # sent = sentence\n",
    "            try:\n",
    "                sent = give_emoji_free_text(sent)\n",
    "                sent = remove_punchuations(sent)\n",
    "                sent = remove_stopwords(sent)\n",
    "                sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "                sent = re.sub(\"\\s+\", \" \", sent)\n",
    "                sent = sent.lower().strip()\n",
    "            except Exception as exp:\n",
    "                print(exp)\n",
    "            # if col == 'title':\n",
    "            #     print(sent)\n",
    "            processed_data.append(sent)\n",
    "\n",
    "        data[col] = processed_data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of category names\n",
    "def get_vohe(X_train, X_validation, X_test, col_name):\n",
    "    \"\"\"\n",
    "    Get one hot encoded features\n",
    "    \"\"\"\n",
    "    vect = CountVectorizer()\n",
    "    tr_ohe = vect.fit_transform(X_train[col_name].values)\n",
    "    vl_ohe = vect.transform(X_validation[col_name].values)\n",
    "    te_ohe = vect.transform(X_test[col_name].values)\n",
    "\n",
    "    return tr_ohe, vl_ohe, te_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vtext_encodings(X_train, X_validation, X_test, col_name, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Get TFIDF encodings with max_features capped at 1M\n",
    "    \"\"\"\n",
    "    vect = TfidfVectorizer(min_df=10, ngram_range=(min_val, max_val), max_features=1000000)\n",
    "    tr_text = vect.fit_transform(X_train[col_name].values)\n",
    "    vl_text = vect.transform(X_validation[col_name].values)\n",
    "    te_text = vect.transform(X_test[col_name].values)\n",
    "\n",
    "    return tr_text, vl_text, te_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_and_validation_image_features(X_train, X_validation):\n",
    "    loaded_features = np.load(image_features_path, allow_pickle=True)\n",
    "    features = list(loaded_features['arr_0'])\n",
    "    feature_dict = dict()\n",
    "    for feature in features:\n",
    "        feature_dict[feature[2]] = [feature[1]]\n",
    "    train_features_in_order = []\n",
    "    validation_features_in_order = []\n",
    "    for index, row in X_train.iterrows():\n",
    "        try:\n",
    "            train_features_in_order.insert(index, feature_dict[row.uniq_id])\n",
    "        except Exception as exp:\n",
    "            print(exp)\n",
    "            img = images_path + row['uniq_id'] + '_0.jpg'\n",
    "            feature_vector = IU.CreateImageFeatureVector(img)\n",
    "            train_features_in_order.insert(index, \n",
    "    for index, row in X_validation.iterrows():\n",
    "        validation_features_in_order.insert(index, feature_dict[row.uniq_id])\n",
    "    return train_features_in_order, validation_features_in_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_features_to_testdf(test_df):\n",
    "#     test_features_in_order = []\n",
    "#     for index, row in test_df.iterrows():\n",
    "#         img = images_path + row['uniq_id'] + '_0.jpg'\n",
    "#         # print(img)\n",
    "#         feature_vector = IU.CreateImageFeaturesVector(img)\n",
    "#         test_features_in_order.insert(index, feature_vector)\n",
    "#     return test_features_in_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encodings_for_features(X_train, X_validation, X_test):\n",
    "    \"\"\"\n",
    "        Get encodings for all the features. Scale and normalize the numerical features. Stack the encoded features horizontally.\n",
    "    \"\"\"\n",
    "    X_train = X_train.fillna('others')\n",
    "    X_test = X_test.fillna('others')\n",
    "    X_validation = X_validation.fillna('others')\n",
    "    X_train = process_text(X_train,\n",
    "                           ['title', 'actual_color', 'product_details', 'complete_the_look', 'specifications', 'care_instruction_0', 'care_instruction_1'])\n",
    "\n",
    "    X_validation = process_text(X_validation,\n",
    "                                ['title', 'actual_color', 'product_details', 'complete_the_look', 'specifications', 'care_instruction_0', 'care_instruction_1'])\n",
    "\n",
    "    X_test = process_text(X_test,\n",
    "                          ['title', 'actual_color', 'product_details', 'complete_the_look', 'specifications', 'care_instruction_0', 'care_instruction_1'])\n",
    "\n",
    "    tr_ohe_brand, vl_ohe_brand, te_ohe_brand = get_vohe(X_train, X_validation, X_test, 'brand')\n",
    "    # tr_ohe_dominant_material, te_ohe_dominant_material = get_vohe(X_train, X_test, 'dominant_material')\n",
    "    tr_ohe_dominant_color, vl_ohe_dominant_color, te_ohe_dominant_color = get_vohe(X_train, X_validation, X_test, 'dominant_color')\n",
    "    tr_ohe_product_type, vl_ohe_product_type, te_ohe_product_type = get_vohe(X_train, X_validation, X_test, 'product_type')\n",
    "    tr_ohe_gender, vl_ohe_gender, te_ohe_gender = get_vohe(X_train, X_validation, X_test, 'gender')\n",
    "    tr_ohe_category_1, vl_ohe_category_1, te_ohe_category_1 = get_vohe(X_train, X_validation, X_test, 'category_1')\n",
    "    tr_ohe_category_0, vl_ohe_category_0, te_ohe_category_0 = get_vohe(X_train, X_validation, X_test, 'category_0')\n",
    "\n",
    "    tr_trans = csr_matrix(\n",
    "        pd.get_dummies(X_train[['cheap_brands', 'expensive_brands', 'luxurious_brands']], sparse=True).values)\n",
    "    vl_trans = csr_matrix(\n",
    "        pd.get_dummies(X_validation[['cheap_brands', 'expensive_brands', 'luxurious_brands']], sparse=True).values)\n",
    "    te_trans = csr_matrix(\n",
    "        pd.get_dummies(X_test[['cheap_brands', 'expensive_brands', 'luxurious_brands']], sparse=True).values)\n",
    "\n",
    "    tr_title, vl_title, te_title = get_vtext_encodings(X_train, X_validation, X_test, 'title', 1, 1)\n",
    "    tr_product_details, vl_product_details, te_product_details = get_vtext_encodings(X_train, X_validation, X_test, 'product_details', 1, 2)\n",
    "    tr_complete_the_look, vl_complete_the_look, te_complete_the_look = get_vtext_encodings(X_train, X_validation, X_test, 'complete_the_look', 1, 2)\n",
    "    tr_specifications, vl_specifications, te_specifications = get_vtext_encodings(X_train, X_validation, X_test, 'specifications', 1, 2)\n",
    "    tr_ohe_dominant_material, vl_ohe_dominant_material, te_ohe_dominant_material = get_vtext_encodings(X_train, X_validation, X_test, 'dominant_material', 1, 1)\n",
    "    tr_ohe_actual_color, vl_ohe_actual_color, te_ohe_actual_color = get_vtext_encodings(X_train, X_validation, X_test, 'actual_color', 1, 1)\n",
    "    tr_ohe_care_instruction_1, vl_ohe_care_instruction_1, te_ohe_care_instruction_1 = get_vtext_encodings(X_train, X_validation, X_test, 'care_instruction_1', 1, 1)\n",
    "    tr_ohe_care_instruction_0, vl_ohe_care_instruction_0, te_ohe_care_instruction_0 = get_vtext_encodings(X_train, X_validation, X_test, 'care_instruction_0', 1, 1)\n",
    "\n",
    "\n",
    "    # Add image feature vectors also to hstack\n",
    "    train_features_in_order, validation_features_in_order = get_test_and_validation_image_features(X_train, X_validation)\n",
    "#     test_features_in_order = add_features_to_testdf(X_test)\n",
    "\n",
    "#     train_data = hstack((tr_ohe_category_0, tr_ohe_category_1, tr_ohe_brand, tr_ohe_dominant_material,\n",
    "#                          tr_ohe_dominant_color, tr_ohe_actual_color, tr_ohe_product_type, tr_trans, tr_ohe_gender,\n",
    "#                          tr_ohe_care_instruction_0, tr_title, tr_product_details, tr_ohe_care_instruction_1,\n",
    "#                          tr_complete_the_look, tr_specifications)).tocsr().astype('float32')\n",
    "#     train_data = np.concatenate((train_data, train_features_in_order))\n",
    "\n",
    "    validation_data = hstack((validation_features_in_order,vl_ohe_category_0, vl_ohe_category_1, vl_ohe_brand, vl_ohe_dominant_material,\n",
    "                         vl_ohe_dominant_color, vl_ohe_actual_color, vl_ohe_product_type, vl_trans, vl_ohe_gender,\n",
    "                         vl_ohe_care_instruction_0, vl_title, vl_product_details, vl_ohe_care_instruction_1,\n",
    "                         vl_complete_the_look, vl_specifications)).tocsr().astype('float32')\n",
    "\n",
    "    validation_data = np.concatenate((validation_features_in_order, validation_data))\n",
    "\n",
    "#     test_data = hstack((test_features_in_order, te_ohe_category_0, te_ohe_category_1, te_ohe_brand, te_ohe_dominant_material,\n",
    "#                         te_ohe_dominant_color, te_ohe_actual_color, te_ohe_product_type, te_trans, te_ohe_gender,\n",
    "#                         te_ohe_care_instruction_0, te_title, te_product_details, te_ohe_care_instruction_1,\n",
    "#                         te_complete_the_look, te_specifications)).tocsr().astype('float32')\n",
    "#     test_data = np.concatenate((test_features_in_order, validation_data))\n",
    "\n",
    "#     return train_data, validation_data, test_data\n",
    "    return validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features_path = os.path.abspath(current_folder + '/..') + '/Data/total_features.npz'\n",
    "data_path = os.path.abspath(current_folder+ '/..') + '/Data'\n",
    "# images_path = \"/ceph/csedu-scratch/project/akaradathodi/Images/\"\n",
    "images_path = os.path.abspath(data_path + '/../../') + '/Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(data_path + \"/test_records.csv\")\n",
    "validation_df = pd.read_csv(data_path + \"/validation_records.csv\")\n",
    "train_df = pd.read_csv(data_path + \"/train_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'5a602b99699215068eed4f79aa1e5c98'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/2677740551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_encodings_for_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/47199554.py\u001b[0m in \u001b[0;36mgenerate_encodings_for_features\u001b[0;34m(X_train, X_validation, X_test)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Add image feature vectors also to hstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain_features_in_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_features_in_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_and_validation_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#     test_features_in_order = add_features_to_testdf(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/44/zdx95fpn3bv669w81970ks_w0000gn/T/ipykernel_40456/2239643910.py\u001b[0m in \u001b[0;36mget_test_and_validation_image_features\u001b[0;34m(X_train, X_validation)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_features_in_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_features_in_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniq_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalidation_features_in_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniq_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '5a602b99699215068eed4f79aa1e5c98'"
     ]
    }
   ],
   "source": [
    "test_data = generate_encodings_for_features(test_df, validation_df, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
