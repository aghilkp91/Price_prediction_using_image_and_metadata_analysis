{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c080177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 07:01:55.160094: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import RetrieveImage as retrieveImage\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from datetime import *\n",
    "import json\n",
    "import pandas as pd\n",
    "import metadata_preprocessing as processing\n",
    "import xgboost as xgb\n",
    "import DeepImageUtils as IU\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack,find\n",
    "import keras\n",
    "\n",
    "from keras import optimizers, callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c2a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.abspath(globals()['_dh'][0] + '/..') + '/Data'\n",
    "# images_path = \"/ceph/csedu-scratch/project/akaradathodi/Images/\"\n",
    "images_path = os.path.abspath(data_path + '/../../') + '/Images/'\n",
    "\n",
    "image_features_path = os.path.abspath(globals()['_dh'][0] + '/..') + '/Data/total_features.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15f2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_xgb_regressor(train_data, y_train, test_data):\n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "    xgb_model.fit(train_data, y_train)\n",
    "    predictions = xgb_model.predict(test_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e638e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_ridge_regression(train_data, y_train, test_data):\n",
    "    ridge_model = Ridge(solver='lsqr', fit_intercept=False)  # solver='lsqr' reduces time to train significantly\n",
    "    ridge_model.fit(train_data, y_train)\n",
    "\n",
    "    predictions = ridge_model.predict(test_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73383d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_light_bgm_regressor(train_data, y_train, test_data):\n",
    "    lgbm_model = LGBMRegressor()\n",
    "    lgbm_model.fit(train_data, y_train)\n",
    "    predictions = lgbm_model.predict(test_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0281b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rmse_mae_r2(predicted_price_xgb, predicted_price_ridge, predicted_price_lightbgm, actual_price):\n",
    "    predicted_price_xgb = np.array(predicted_price_xgb)\n",
    "    predicted_price_ridge = np.array(predicted_price_ridge)\n",
    "    predicted_price_lightbgm = np.array(predicted_price_lightbgm)\n",
    "    actual_price = np.array(actual_price)\n",
    "\n",
    "    results = {\n",
    "            'MSE_xgb': mean_squared_error(predicted_price_xgb, actual_price),\n",
    "            'MAE_xgb': mean_absolute_error(predicted_price_xgb, actual_price),\n",
    "            'R2_xgb': r2_score(predicted_price_xgb, actual_price),\n",
    "            'MSE_ridge': mean_squared_error(predicted_price_ridge, actual_price),\n",
    "            'MAE_ridge': mean_absolute_error(predicted_price_ridge, actual_price),\n",
    "            'R2_ridge': r2_score(predicted_price_ridge, actual_price),\n",
    "            'MSE_lightbgm': mean_squared_error(predicted_price_lightbgm, actual_price),\n",
    "            'MAE_lightbgm': mean_absolute_error(predicted_price_lightbgm, actual_price),\n",
    "            'R2_lightbgm': r2_score(predicted_price_lightbgm, actual_price)\n",
    "        }\n",
    "    with open(data_path + '/output/feature_stacking_clothing_' + datetime.now().strftime(\n",
    "                \"%Y-%m-%d %H:%M:%S\") + '.json', 'w') as file:\n",
    "            json.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e93bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rmse_mae_r2_for_one_method(predicted_price, actual_price):\n",
    "    predicted_price = np.array(predicted_price)\n",
    "    actual_price = np.array(actual_price)\n",
    "\n",
    "    results = {\n",
    "            'MSE': mean_squared_error(predicted_price, actual_price),\n",
    "            'MAE': mean_absolute_error(predicted_price, actual_price),\n",
    "            'R2': r2_score(predicted_price, actual_price),\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7011a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_prices_by_feature_vectors(test_df, validation_df, train_df):\n",
    "    # train_df, validation_df = add_features_to_df(train_df, validation_df)\n",
    "    # test_df = add_features_to_testdf(test_df)\n",
    "    X_test = test_df.drop(['Unnamed: 0', 'care_instructions','images', 'image_0', 'image_1', 'image_2', 'price'],\n",
    "                     axis=1)\n",
    "    y_test = test_df['price']\n",
    "\n",
    "    y_train = train_df['price']\n",
    "    X_train = train_df.drop(['Unnamed: 0','images','care_instructions','image_0','image_1','image_2','price'], axis = 1)\n",
    "\n",
    "    y_validation = validation_df['price']\n",
    "    X_validation = validation_df.drop(\n",
    "        ['Unnamed: 0', 'images', 'care_instructions', 'image_0', 'image_1', 'image_2', 'price'], axis=1)\n",
    "\n",
    "    train_data, validation_data, test_data = generate_encodings_for_features(X_train, X_validation, X_test)\n",
    "\n",
    "\n",
    "    print('--Final Data Matrix--')\n",
    "    print(train_data.shape, y_test.shape)\n",
    "        # print(test_data.shape, row['price'].shape)\n",
    "        #do price prediction and return the price\n",
    "\n",
    "    return train_data, validation_data, test_data, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b6d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    # return emoji.get_emoji_regexp().sub(r'', text)\n",
    "    pattern = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825e25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punchuations(sentence):\n",
    "    # return text.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "    regular_punct = list(string.punctuation)\n",
    "\n",
    "    for punc in regular_punct:\n",
    "        if punc in sentence:\n",
    "            sentence = sentence.replace(punc, ' ')\n",
    "\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a98fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contract_words(text):\n",
    "    try:\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    except Exception as exp:\n",
    "        print(exp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580276c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # return ' '.join([word for word in text if word not in stopwords.words('english')])\n",
    "    return ' '.join(e for e in text.split() if e not in STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11e95c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(data, cols):\n",
    "    # print(data.columns.values.tolist())\n",
    "    for col in cols:\n",
    "\n",
    "        processed_data = []\n",
    "\n",
    "        for sentence in data[col].values:\n",
    "            sent = remove_contract_words(sentence)\n",
    "            # sent = sentence\n",
    "            try:\n",
    "                sent = give_emoji_free_text(sent)\n",
    "                sent = remove_punchuations(sent)\n",
    "                sent = remove_stopwords(sent)\n",
    "                sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "                sent = re.sub(\"\\s+\", \" \", sent)\n",
    "                sent = sent.lower().strip()\n",
    "            except Exception as exp:\n",
    "                print(exp)\n",
    "            # if col == 'title':\n",
    "            #     print(sent)\n",
    "            processed_data.append(sent)\n",
    "\n",
    "        data[col] = processed_data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ddab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vohe(X_train, X_validation, X_test, col_name):\n",
    "    \"\"\"\n",
    "    Get one hot encoded features\n",
    "    \"\"\"\n",
    "    vect = CountVectorizer()\n",
    "    tr_ohe = vect.fit_transform(X_train[col_name].values)\n",
    "    vl_ohe = vect.transform(X_validation[col_name].values)\n",
    "    te_ohe = vect.transform(X_test[col_name].values)\n",
    "\n",
    "    return tr_ohe, vl_ohe, te_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f321f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf word embeddings\n",
    "def get_vtext_encodings(X_train, X_validation, X_test, col_name, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Get TFIDF encodings with max_features capped at 1M\n",
    "    \"\"\"\n",
    "    vect = TfidfVectorizer(min_df=10, ngram_range=(min_val, max_val), max_features=1000000)\n",
    "    tr_text = vect.fit_transform(X_train[col_name].values)\n",
    "    vl_text = vect.transform(X_validation[col_name].values)\n",
    "    te_text = vect.transform(X_test[col_name].values)\n",
    "\n",
    "    return tr_text, vl_text, te_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fd67c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encodings_for_features(X_train, X_validation, X_test):\n",
    "    \"\"\"\n",
    "        Get encodings for all the features. Scale and normalize the numerical features. Stack the encoded features horizontally.\n",
    "    \"\"\"\n",
    "    X_train = X_train.fillna('others')\n",
    "    X_test = X_test.fillna('others')\n",
    "    X_validation = X_validation.fillna('others')\n",
    "    X_train = process_text(X_train,\n",
    "                           ['title', 'actual_color', 'product_details', 'complete_the_look', 'specifications', 'care_instruction_0', 'care_instruction_1'])\n",
    "\n",
    "    X_validation = process_text(X_validation,\n",
    "                                ['title', 'actual_color', 'product_details', 'complete_the_look', 'specifications', 'care_instruction_0', 'care_instruction_1'])\n",
    "\n",
    "    X_test = process_text(X_test,\n",
    "                          ['title', 'actual_color', 'product_details', 'complete_the_look', 'specifications', 'care_instruction_0', 'care_instruction_1'])\n",
    "\n",
    "    tr_ohe_brand, vl_ohe_brand, te_ohe_brand = get_vohe(X_train, X_validation, X_test, 'brand')\n",
    "    # tr_ohe_dominant_material, te_ohe_dominant_material = get_vohe(X_train, X_test, 'dominant_material')\n",
    "    tr_ohe_dominant_color, vl_ohe_dominant_color, te_ohe_dominant_color = get_vohe(X_train, X_validation, X_test, 'dominant_color')\n",
    "    tr_ohe_product_type, vl_ohe_product_type, te_ohe_product_type = get_vohe(X_train, X_validation, X_test, 'product_type')\n",
    "    tr_ohe_gender, vl_ohe_gender, te_ohe_gender = get_vohe(X_train, X_validation, X_test, 'gender')\n",
    "    tr_ohe_category_1, vl_ohe_category_1, te_ohe_category_1 = get_vohe(X_train, X_validation, X_test, 'category_1')\n",
    "    tr_ohe_category_0, vl_ohe_category_0, te_ohe_category_0 = get_vohe(X_train, X_validation, X_test, 'category_0')\n",
    "\n",
    "    tr_trans = csr_matrix(\n",
    "        pd.get_dummies(X_train[['cheap_brands', 'expensive_brands', 'luxurious_brands']], sparse=True).values)\n",
    "    vl_trans = csr_matrix(\n",
    "        pd.get_dummies(X_validation[['cheap_brands', 'expensive_brands', 'luxurious_brands']], sparse=True).values)\n",
    "    te_trans = csr_matrix(\n",
    "        pd.get_dummies(X_test[['cheap_brands', 'expensive_brands', 'luxurious_brands']], sparse=True).values)\n",
    "\n",
    "    tr_title, vl_title, te_title = get_vtext_encodings(X_train, X_validation, X_test, 'title', 1, 1)\n",
    "    tr_product_details, vl_product_details, te_product_details = get_vtext_encodings(X_train, X_validation, X_test, 'product_details', 1, 2)\n",
    "    tr_complete_the_look, vl_complete_the_look, te_complete_the_look = get_vtext_encodings(X_train, X_validation, X_test, 'complete_the_look', 1, 2)\n",
    "    tr_specifications, vl_specifications, te_specifications = get_vtext_encodings(X_train, X_validation, X_test, 'specifications', 1, 2)\n",
    "    tr_ohe_dominant_material, vl_ohe_dominant_material, te_ohe_dominant_material = get_vtext_encodings(X_train, X_validation, X_test, 'dominant_material', 1, 1)\n",
    "    tr_ohe_actual_color, vl_ohe_actual_color, te_ohe_actual_color = get_vtext_encodings(X_train, X_validation, X_test, 'actual_color', 1, 1)\n",
    "    tr_ohe_care_instruction_1, vl_ohe_care_instruction_1, te_ohe_care_instruction_1 = get_vtext_encodings(X_train, X_validation, X_test, 'care_instruction_1', 1, 1)\n",
    "    tr_ohe_care_instruction_0, vl_ohe_care_instruction_0, te_ohe_care_instruction_0 = get_vtext_encodings(X_train, X_validation, X_test, 'care_instruction_0', 1, 1)\n",
    "\n",
    "\n",
    "    train_data = hstack((tr_ohe_category_0, tr_ohe_category_1, tr_ohe_brand, tr_ohe_dominant_material,\n",
    "                         tr_ohe_dominant_color, tr_ohe_actual_color, tr_ohe_product_type, tr_trans, tr_ohe_gender,\n",
    "                         tr_ohe_care_instruction_0, tr_title, tr_product_details, tr_ohe_care_instruction_1,\n",
    "                         tr_complete_the_look, tr_specifications)).tocsr().astype('float32')\n",
    "\n",
    "    validation_data = hstack((vl_ohe_category_0, vl_ohe_category_1, vl_ohe_brand, vl_ohe_dominant_material,\n",
    "                         vl_ohe_dominant_color, vl_ohe_actual_color, vl_ohe_product_type, vl_trans, vl_ohe_gender,\n",
    "                         vl_ohe_care_instruction_0, vl_title, vl_product_details, vl_ohe_care_instruction_1,\n",
    "                         vl_complete_the_look, vl_specifications)).tocsr().astype('float32')\n",
    "\n",
    "    # validation_data = np.concatenate((validation_features_in_order, validation_data))\n",
    "\n",
    "    test_data = hstack((te_ohe_category_0, te_ohe_category_1, te_ohe_brand, te_ohe_dominant_material,\n",
    "                        te_ohe_dominant_color, te_ohe_actual_color, te_ohe_product_type, te_trans, te_ohe_gender,\n",
    "                        te_ohe_care_instruction_0, te_title, te_product_details, te_ohe_care_instruction_1,\n",
    "                        te_complete_the_look, te_specifications)).tocsr().astype('float32')\n",
    "    # test_data = np.concatenate((test_features_in_order, validation_data))\n",
    "\n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67873f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                            0\n",
       "uniq_id                                7b5b9b1854e0ae2edcb99e7dd839f06a\n",
       "product_id                                                      8350233\n",
       "size                                                                XXL\n",
       "variant_sku                                                 8350233_XXL\n",
       "brand                                                           Sangria\n",
       "care_instructions                                             Hand Wash\n",
       "dominant_material                                                   NaN\n",
       "title                 Sangria Women Red & Orange Printed Straight Fi...\n",
       "actual_color                                               Red | Orange\n",
       "dominant_color                                                      Red\n",
       "product_type                                                        NaN\n",
       "images                http://assets.myntassets.com/v1/assets/images/...\n",
       "body                  Minimal yet stylish, this orange kurta from Sa...\n",
       "product_details       Minimal yet stylish, this orange kurta from Sa...\n",
       "complete_the_look                                                   NaN\n",
       "category                  Clothing/Women/Kurtas/Sangria/More by Sangria\n",
       "price                                                               999\n",
       "gender                                                            Women\n",
       "specifications        Shape : Straight | Sleeve Styling : Regular Sl...\n",
       "category_0                                                     Clothing\n",
       "category_1                                                       Kurtas\n",
       "image_0               http://assets.myntassets.com/v1/assets/images/...\n",
       "image_1                http://assets.myntassets.com/v1/assets/images...\n",
       "image_2                http://assets.myntassets.com/v1/assets/images...\n",
       "luxurious_brands                                                  False\n",
       "expensive_brands                                                  False\n",
       "cheap_brands                                                       True\n",
       "care_instruction_0                                            Hand Wash\n",
       "care_instruction_1                                                  NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(data_path + \"/test_records_cloth.csv\")\n",
    "validation_df = pd.read_csv(data_path + \"/validation_records_cloth.csv\")\n",
    "train_df = pd.read_csv(data_path + \"/train_records_cloth.csv\")\n",
    "\n",
    "predicted_price_xgb = []\n",
    "predicted_price_ridge = []\n",
    "predicted_price_lightbgm = []\n",
    "actual_price = []\n",
    "\n",
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b475a960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Final Data Matrix--\n",
      "(6332, 5724) (2108,)\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data, y_train, y_validation, y_test = get_predicted_prices_by_feature_vectors(test_df, validation_df, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b552b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price_xgb.append(run_on_xgb_regressor(train_data, y_train, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a3c1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price_ridge.append(run_on_ridge_regression(train_data, y_train, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99ecb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price_lightbgm.append(run_on_light_bgm_regressor(train_data, y_train, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0398366",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_rmse_mae_r2(predicted_price_xgb[0], predicted_price_ridge[0], predicted_price_lightbgm[0], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0710eb8",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92bbba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rmsle explained for n_estimators = 50 is {'MSE': 760531.9439087531, 'MAE': 548.4593718673727, 'R2': 0.6181427874716697}\n",
      "Test rmsle explained for n_estimators = 100 is {'MSE': 672767.9068108031, 'MAE': 505.4893763926571, 'R2': 0.7015190163114022}\n",
      "Test rmsle explained for n_estimators = 300 is {'MSE': 612337.8294105135, 'MAE': 459.15013331826043, 'R2': 0.7489999101545507}\n",
      "Test rmsle explained for n_estimators = 500 is {'MSE': 602669.811906893, 'MAE': 449.52933617471706, 'R2': 0.7578924954366755}\n",
      "Test rmsle explained for n_estimators = 1000 is {'MSE': 607084.7323634732, 'MAE': 447.39324450348767, 'R2': 0.7598254359099315}\n"
     ]
    }
   ],
   "source": [
    "#values of these parameters are taken randomly\n",
    "xgb_params = {'gamma': 0.2, 'reg_lambda': 0.5, 'learning_rate': 0.07,\n",
    "              'subsample': 0.7, 'colsample_bytree': 0.9, 'reg_alpha': 0.6,\n",
    "              'max_depth': 6}\n",
    "\n",
    "estimators = [50, 100, 300, 500, 1000]\n",
    "\n",
    "for i in estimators:\n",
    "    regr = xgb.XGBRegressor(n_estimators=i)\n",
    "    regr.set_params(**xgb_params)\n",
    "\n",
    "    regr.fit(train_data, y_train)\n",
    "    y_pred = regr.predict(validation_data)\n",
    "    \n",
    "    print('Test rmsle explained for n_estimators = {} is {}'.format(i, str(find_rmse_mae_r2_for_one_method(y_pred, y_validation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53390416",
   "metadata": {},
   "source": [
    "### XGB with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4fb3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'gamma': 0.2, 'reg_lambda': 0.5, 'learning_rate': 0.07,\n",
    "              'subsample': 0.7, 'colsample_bytree': 0.9, 'reg_alpha': 0.6,\n",
    "              'max_depth': 6}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators = 500)\n",
    "xgb_model.set_params(**xgb_params)\n",
    "\n",
    "xgb_model.fit(train_data, y_train)\n",
    "\n",
    "predictions_xgb = xgb_model.predict(test_data)\n",
    "result = find_rmse_mae_r2_for_one_method(predictions_xgb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e22ed37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 492853.69426420436, 'MAE': 432.2104842531613, 'R2': 0.7766914993990468}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c5eb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + '/output/feature_stacking_xgb_best_hyperparameters_Clothing_' + datetime.now().strftime(\n",
    "                \"%Y-%m-%d-%H:%M:%S\") + '.json', 'w') as file:\n",
    "            json.dump(result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83871a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}